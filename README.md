# REER Ã— DSPy Ã— MLX Social Posting Pack

[![Python 3.11+](https://img.shields.io/badge/python-3.11%2B-blue.svg)](https://www.python.org/downloads/)
[![DSPy 3.0+](https://img.shields.io/badge/DSPy-3.0%2B-green.svg)](https://github.com/stanfordnlp/dspy)
[![MLX](https://img.shields.io/badge/MLX-optimized-orange.svg)](https://github.com/ml-explore/mlx)

## ğŸš€ What is REER?

REER (Retrieval-Enhanced Evolutionary Refinement) is an advanced algorithm that optimizes AI-generated text through iterative trajectory search. It works by finding optimal "reasoning chains" between inputs and outputs, minimizing perplexity to create more natural, coherent text.

```
Input (x) â†’ Reasoning Chain (z) â†’ Output (y)
```

The system iteratively optimizes `z` to minimize the perplexity of the entire sequence (x+z+y), resulting in:
- More coherent responses
- Better factual consistency
- Improved logical flow
- Natural language that "feels right"

## âœ¨ Key Features

### Core Capabilities
- **ğŸ”„ REER Trajectory Search**: Evolutionary refinement using perplexity-guided optimization
- **ğŸ¯ GEPA Optimization**: Automatic DSPy prompt tuning with reflection-based learning (GPT-4)
- **âš¡ MLX Acceleration**: Apple Silicon optimized for local, private inference (M1/M2/M3)
- **â˜ï¸ Hybrid Backends**: Seamless switching between local (MLX) and cloud (Together API)
- **ğŸ“Š Sliding Window**: Handles long contexts (2048 token window, 1024 stride)

### Social Media Features
- **ğŸ“± Multi-Platform Support**: Twitter/X, Facebook, Instagram, LinkedIn, Discord, Slack
- **â° Smart Scheduling**: Automated posting with optimal timing
- **ğŸ”€ Content Variants**: A/B testing ready with multiple variations
- **ğŸš€ Async Architecture**: High-performance async/await patterns
- **ğŸ“ˆ Analytics Ready**: Track engagement and optimize content

## ğŸ“‹ Requirements

### Hardware
- **For MLX Backend**: Apple Silicon Mac (M1/M2/M3)
- **For Cloud Backend**: Any system with internet connection

### Software
- Python 3.11+
- DSPy 3.0+
- MLX 0.19+ (for Apple Silicon)

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/reer-team/reer_mlx.git
cd reer_mlx
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### 1. Local MLX (No API Key Required)

Run trajectory optimization locally on Apple Silicon:

```bash
python scripts/reer_synthesize.py examples/qa.jsonl \
  --backend mlx \
  --model "mlx-community/Llama-3.2-1B-Instruct" \
  --x-field "question" \
  --y-field "answer" \
  --limit 5
```

### 2. Together API (Cloud)

Use Together's API for non-Apple Silicon systems:

```bash
export TOGETHER_API_KEY="your-key-here"

python scripts/reer_synthesize.py examples/qa.jsonl \
  --backend together \
  --model "meta-llama/Llama-3-8b-hf" \
  --x-field "question" \
  --y-field "answer" \
  --limit 5
```

âš ï¸ **Note**: Together backend requires models with `loglikelihood` support. The system validates this at startup.

### 3. GEPA Optimization

Optimize DSPy modules with GEPA (requires OpenAI API for reflection):

```python
from dspy_program.gepa_runner import run_gepa

# Set API key for GPT-4 reflection
import os
os.environ["OPENAI_API_KEY"] = "sk-..."

# Training examples
train = [
    {"topic": "AI productivity", "audience": "tech professionals"},
    {"topic": "Startup growth", "audience": "founders"},
]

# Run GEPA optimization
optimized = run_gepa(
    train,
    gen_model="mlx-community/Llama-3.2-3B-Instruct-4bit",
    reflection_model="gpt-4o",
    auto="light",  # light/medium/heavy
)

# Use optimized module
result = optimized(topic="AI tools", audience="developers")
print(result.get("post"))
```

## ğŸ”‘ Environment Variables

### Essential for GEPA
```bash
export OPENAI_API_KEY="sk-..."  # GPT-4 for reflection
```

### For Together Backend
```bash
export TOGETHER_API_KEY="..."   # Must support loglikelihood
```

### For Social Posting (Optional)
```bash
export TWITTER_API_KEY="..."
export TWITTER_API_SECRET="..."
export FACEBOOK_ACCESS_TOKEN="..."
export INSTAGRAM_ACCESS_TOKEN="..."
export DISCORD_BOT_TOKEN="..."
export SLACK_BOT_TOKEN="..."
```

### For Tracking (Optional)
```bash
export WANDB_API_KEY="..."  # Weights & Biases tracking
```

## ğŸ“ Project Structure

```
reer_mlx/
â”œâ”€â”€ reer/                    # Core REER implementation
â”‚   â””â”€â”€ trajectory_search.py # Trajectory optimization algorithm
â”œâ”€â”€ dspy_program/           # DSPy modules and GEPA
â”‚   â”œâ”€â”€ gepa_runner.py     # GEPA optimizer integration
â”‚   â””â”€â”€ pipeline.py        # DSPy pipeline components
â”œâ”€â”€ tools/                  # Utilities
â”‚   â””â”€â”€ ppl_eval.py        # PPL evaluators (MLX & Together)
â”œâ”€â”€ scripts/                # CLI tools
â”‚   â”œâ”€â”€ reer_synthesize.py # Main trajectory synthesis
â”‚   â”œâ”€â”€ reer_benchmark.py  # Benchmarking tool
â”‚   â””â”€â”€ social_*.py        # Social media scripts
â””â”€â”€ tests/                  # Test suites
```

## ğŸ§ª How It Works

### 1. Trajectory Search Algorithm

The REER algorithm optimizes reasoning chains through:
1. **Segmentation**: Breaks reasoning into manageable chunks
2. **Candidate Generation**: Creates variations for each segment
3. **PPL Evaluation**: Measures naturalness using perplexity
4. **Selection**: Keeps best candidates based on PPL scores
5. **Iteration**: Repeats until convergence or max iterations

### 2. PPL Evaluation Backends

#### MLX (Local)
- Runs entirely on-device using Apple Silicon
- Sliding window approach for long contexts
- No API costs or privacy concerns

#### Together (Cloud)
- Uses API for exact loglikelihood computation
- Supports any hardware platform
- Requires API credits

### 3. GEPA Optimization

GEPA (Gradient-Estimation-based Prompt Adaptation) automatically:
- Analyzes failures using reflection LM (GPT-4)
- Generates improved prompts based on feedback
- Evaluates candidates in parallel
- Converges on optimal configuration

## ğŸ“Š Benchmarks

Performance on M2 Max with Llama-3.2-1B:
- **Throughput**: ~50 tokens/sec (MLX backend)
- **Memory**: 2-4GB for 1B model
- **Optimization**: 8 iterations typical convergence
- **PPL Improvement**: 15-30% average reduction

## ğŸ§ª Testing

Run the test suite:

```bash
# All tests
pytest tests/

# Specific components
pytest tests/test_sliding_window.py  # MLX sliding window
pytest tests/test_together_validation.py  # Together backend
pytest tests/integration/test_gepa_tuning.py  # GEPA optimization
```

## ğŸ“– Documentation

- [Architecture Overview](docs/ARCHITECTURE.md)
- [CLI Usage Guide](CLI_USAGE_GUIDE.md)
- [DSPy Implementation](DSPy_IMPLEMENTATION_SUMMARY.md)
- [API Reference](docs/api/)

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [DSPy](https://github.com/stanfordnlp/dspy) - Stanford NLP's prompt programming framework
- [MLX](https://github.com/ml-explore/mlx) - Apple's machine learning framework
- [Together AI](https://together.ai) - LLM inference platform
- REER paper authors for the trajectory search algorithm

## âš ï¸ Troubleshooting

### Together Backend Issues
If you see "AttributeError: 'LM' object has no attribute 'loglikelihood'":
- Your Together model doesn't support loglikelihood
- Switch to MLX backend or use a different model

### MLX Memory Issues
For large models on limited RAM:
- Use 4-bit quantized models (e.g., `*-4bit` variants)
- Reduce window_size parameter
- Close other applications

### GEPA Reflection Errors
If GEPA fails during reflection:
- Verify OPENAI_API_KEY is set correctly
- Check GPT-4 API quota/credits
- Try with `auto="light"` for fewer API calls